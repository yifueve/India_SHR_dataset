{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from kmodes.kprototypes import KPrototypes\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read the subcritical coal power plant data\n",
    "\n",
    "sub_critical = pd.read_csv('CEEW_subcritical_with_ws_price.csv')\n",
    "\n",
    "all_heat_rate = sub_critical['Actual SHR']* 3.96567/1000\n",
    "all_capacity  = sub_critical['Capacity']\n",
    "all_age       = sub_critical['Age']\n",
    "all_region     = sub_critical['Region']\n",
    "all_PLF        = sub_critical['Actual avg PLF']\n",
    "all_water_stress = sub_critical['bws_score']\n",
    "all_price = sub_critical['coal_price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Find the correlation between the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   heat_rate  capacity       age       PLF  water stress  price  region_ER  \\\n",
      "0  10.971691     600.0  1.670089  0.328097      0.480176   1.72      False   \n",
      "1  10.979451     600.0  2.116359  0.332492      0.373747   3.43      False   \n",
      "2  10.321183     600.0  3.425051  0.864876      0.250206   2.04      False   \n",
      "3  10.628501     600.0  2.338125  0.635221      0.139977   1.59      False   \n",
      "4  10.816011     600.0  4.175222  0.504753      0.591333   2.04      False   \n",
      "\n",
      "   region_NER  region_NR  region_SR  region_WR  \n",
      "0       False      False      False       True  \n",
      "1       False      False       True      False  \n",
      "2       False      False       True      False  \n",
      "3       False      False      False       True  \n",
      "4       False       True      False      False  \n"
     ]
    }
   ],
   "source": [
    "data = pd.DataFrame({'heat_rate': all_heat_rate,\n",
    "                     'capacity': all_capacity,\n",
    "                     'age': all_age, \n",
    "                     'PLF': all_PLF,\n",
    "                     'water stress': all_water_stress,\n",
    "                     'region': all_region,\n",
    "                     'price': all_price\n",
    "                     })\n",
    "\n",
    "# change the state to one hot encoding\n",
    "data = pd.get_dummies(data, columns=['region']) # aggregate the states here. \n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Fit the prediction model using data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('heat_rate', axis=1)\n",
    "y = data['heat_rate']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# add constant\n",
    "X_train = np.c_[np.ones(X_train.shape[0]), X_train]\n",
    "X_test = np.c_[np.ones(X_test.shape[0]), X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['heat_rate', 'capacity', 'age', 'PLF', 'water stress', 'price',\n",
      "       'region_ER', 'region_NER', 'region_NR', 'region_SR', 'region_WR'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# show the unique states\n",
    "print(data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "models = {\n",
    "    'GradientBoostingRegressor': GradientBoostingRegressor(),\n",
    "    'RandomForestRegressor': RandomForestRegressor(), # depth\n",
    "    'DecisionTreeRegressor': DecisionTreeRegressor(), # depth\n",
    "    'XGBRegressor': XGBRegressor(), \n",
    "    'LinearRegression': LinearRegression(), # no hyperparameters\n",
    "    'RidgeRegression': Ridge(), # alpha\n",
    "    'SVR': SVR(), # \n",
    "    'KNeighborsRegressor': KNeighborsRegressor(),\n",
    "}\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Cross validation\n",
    "kf = KFold(n_splits=5)\n",
    "res = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing hyperparameter search for GradientBoostingRegressor\n",
      "Fitting 5 folds for each of 256 candidates, totalling 1280 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing hyperparameter search for RandomForestRegressor\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Performing hyperparameter search for DecisionTreeRegressor\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Performing hyperparameter search for XGBRegressor\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "Performing hyperparameter search for LinearRegression\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Performing hyperparameter search for RidgeRegression\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Performing hyperparameter search for SVR\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Performing hyperparameter search for KNeighborsRegressor\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "scorer = make_scorer(mean_absolute_percentage_error, greater_is_better=False)\n",
    "\n",
    "# Hyperparameter grids for each model\n",
    "param_grids = {\n",
    "    'GradientBoostingRegressor': {\n",
    "        'n_estimators': [100, 150, 200, 300],\n",
    "        'learning_rate': [1, 0.5, 0.1, 0.01],\n",
    "        'max_depth': [10, 15, 30, 40, ],\n",
    "        'loss': ['squared_error', 'absolute_error', 'huber', 'quantile']\n",
    "    },\n",
    "    'RandomForestRegressor': {\n",
    "        'max_depth': [None]\n",
    "    },\n",
    "    'DecisionTreeRegressor': {\n",
    "        'max_depth': [None]\n",
    "    },\n",
    "    'XGBRegressor': {\n",
    "        'n_estimators': [100, 150, 300, 400],\n",
    "        'learning_rate': [1, 0.5, 0.1, 0.01],\n",
    "        'max_depth': [2, 5, 10],\n",
    "    },\n",
    "    'LinearRegression': {},\n",
    "    'RidgeRegression': {\n",
    "        'alpha': [0.1, 1, 10, 100]\n",
    "    },\n",
    "    'SVR': {\n",
    "        'C': [0.01, 0.1, 1, ],\n",
    "        'gamma': [0.01, 0.1, 1, 10],\n",
    "        'kernel': ['linear', 'rbf', 'poly']\n",
    "    },\n",
    "    'KNeighborsRegressor': {\n",
    "        'n_neighbors': [2, 3, 5, 6],\n",
    "        'weights': ['uniform', 'distance']\n",
    "    },\n",
    "}\n",
    "\n",
    "# Perform hyperparameter search\n",
    "param_search_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    # if (name != 'LGBMRegressor'):\n",
    "    #     continue\n",
    "    print(f\"Performing hyperparameter search for {name}\")\n",
    "\n",
    "    param_grid = param_grids.get(name, {})  # Get corresponding param grid, or an empty dict\n",
    "\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=kf, scoring=scorer, n_jobs=-1, verbose=1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    param_search_results[name] = {\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'best_score': grid_search.best_score_,\n",
    "        'cv_results': grid_search.cv_results_\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingRegressor {'learning_rate': 0.1, 'loss': 'absolute_error', 'max_depth': 15, 'n_estimators': 200} -0.008004271191169745\n",
      "RandomForestRegressor {'max_depth': None} -0.010236167071831389\n",
      "DecisionTreeRegressor {'max_depth': None} -0.012403305607540122\n",
      "XGBRegressor {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 400} -0.009292294672785317\n",
      "LinearRegression {} -0.009387017514369642\n",
      "RidgeRegression {'alpha': 1} -0.009383860544574286\n",
      "SVR {'C': 0.1, 'gamma': 0.01, 'kernel': 'linear'} -0.008142652208678644\n",
      "KNeighborsRegressor {'n_neighbors': 2, 'weights': 'distance'} -0.011820006937484936\n"
     ]
    }
   ],
   "source": [
    "# show the best hyperparameters for each model in a table\n",
    "for name, result in param_search_results.items():\n",
    "    print(name, result['best_params'], result['best_score'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "colabcode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0976288d17e384aa8dbcecc74387b05117c6fe79d3a9aebdf73c96c0abdfff35"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
